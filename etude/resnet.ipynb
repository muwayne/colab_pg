{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Device configuration for Apple chips\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "def data_loader(\n",
    "    dataset_name,\n",
    "    data_dir,\n",
    "    batch_size,\n",
    "    random_seed=42,\n",
    "    valid_size=0.1,\n",
    "    shuffle=True,\n",
    "    test=False,\n",
    "):\n",
    "    # Dataset-specific normalization values\n",
    "    if dataset_name == \"CIFAR10\":\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2023, 0.1994, 0.2010],\n",
    "        )\n",
    "        resize_dim = (224, 224)  # Original CIFAR10 size is 32x32\n",
    "    elif dataset_name == \"CIFAR100\":\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.5071, 0.4867, 0.4408],\n",
    "            std=[0.2675, 0.2565, 0.2761],\n",
    "        )\n",
    "        resize_dim = (224, 224)  # Original CIFAR100 size is 32x32\n",
    "    elif dataset_name == \"TinyImageNet\":\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        )\n",
    "        resize_dim = (64, 64)  # Original Tiny ImageNet size is 64x64\n",
    "    elif dataset_name == \"StanfordDogs\":\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        )\n",
    "        resize_dim = (\n",
    "            224,\n",
    "            224,\n",
    "        )  # Images will be resized to 224x224 for ResNet-like models\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported.\")\n",
    "\n",
    "    # Define transformations: Resize and normalize\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(resize_dim),  # Resize depending on the dataset\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Choose the dataset\n",
    "    if dataset_name in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "        dataset_cls = datasets.__dict__[dataset_name]\n",
    "    elif dataset_name == \"TinyImageNet\":\n",
    "        dataset_cls = datasets.ImageFolder  # Tiny ImageNet is structured with folders\n",
    "    elif dataset_name == \"StanfordDogs\":\n",
    "        dataset_cls = (\n",
    "            datasets.ImageFolder\n",
    "        )  # Stanford Dogs is also structured with folders\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported.\")\n",
    "\n",
    "    # Handle test mode\n",
    "    if test:\n",
    "        if dataset_name in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "            dataset = dataset_cls(\n",
    "                root=data_dir,\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transform,\n",
    "            )\n",
    "        else:\n",
    "            dataset = dataset_cls(\n",
    "                root=os.path.join(\n",
    "                    data_dir, \"test\" if dataset_name == \"TinyImageNet\" else \"val\"\n",
    "                ),\n",
    "                transform=transform,\n",
    "            )\n",
    "\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        return test_loader\n",
    "\n",
    "    # Load train dataset\n",
    "    if dataset_name in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "        train_dataset = dataset_cls(\n",
    "            root=data_dir,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform,\n",
    "        )\n",
    "\n",
    "        valid_dataset = dataset_cls(\n",
    "            root=data_dir,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform,\n",
    "        )\n",
    "    else:\n",
    "        train_dataset = dataset_cls(\n",
    "            root=os.path.join(data_dir, \"train\"),\n",
    "            transform=transform,\n",
    "        )\n",
    "\n",
    "        valid_dataset = dataset_cls(\n",
    "            root=os.path.join(data_dir, \"train\"),\n",
    "            transform=transform,\n",
    "        )\n",
    "\n",
    "    # Split the train dataset into train and validation\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    print(f\"Train size: {len(train_idx)}\")\n",
    "    print(f\"Validation size: {len(valid_idx)}\")\n",
    "\n",
    "    # Create samplers for train and validation\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler\n",
    "    )\n",
    "\n",
    "    return (train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "log_dir = \"./runs/resnet_experiment_\" + time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "\n",
    "# Hook function to capture the activations\n",
    "def hook_fn(module, input, output):\n",
    "    writer.add_histogram(f\"{module.__class__.__name__}_activations\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock_A(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock_A, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        # automatically create downsample layer if needed\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Register hooks on conv1 and conv2 to visualize intermediate activations\n",
    "        # self.conv1[0].register_forward_hook(hook_fn)\n",
    "        # self.conv2[0].register_forward_hook(hook_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock_B(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bottleneck_rate=4, stride=1):\n",
    "        super(ResidualBlock_B, self).__init__()\n",
    "\n",
    "        # Calculate the number of mid_channels using the bottleneck rate\n",
    "        mid_channels = out_channels // bottleneck_rate\n",
    "\n",
    "        # 1x1 convolution (to reduce dimensionality)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, mid_channels, kernel_size=1, stride=stride, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 3x3 convolution (spatial convolution)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                mid_channels,\n",
    "                mid_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 1x1 convolution (to restore dimensionality)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        # If the input and output sizes don't match, we need a downsample layer\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Register hooks on conv1 and conv2 to visualize intermediate activations\n",
    "        # self.conv1[0].register_forward_hook(hook_fn)\n",
    "        # self.conv2[0].register_forward_hook(hook_fn)\n",
    "        # self.conv3[0].register_forward_hook(hook_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        # Forward pass through the three convolutional layers\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        # Apply downsample if needed\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        # Add the residual (skip connection)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, residual_block, num_classes=10):\n",
    "        super(ResNet34, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(residual_block, 64, 64, 3)\n",
    "        self.layer1 = self._make_layer(residual_block, 64, 128, 4, init_stride=2)\n",
    "        self.layer2 = self._make_layer(residual_block, 128, 256, 6, init_stride=2)\n",
    "        self.layer3 = self._make_layer(residual_block, 256, 512, 3, init_stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "        self, residual_block, in_channels, out_channels, num_blocks, init_stride=1\n",
    "    ):\n",
    "        layers = [residual_block(in_channels, out_channels, init_stride)]\n",
    "        for i in range(1, num_blocks):\n",
    "            layers.append(residual_block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, residual_block, num_classes=10):\n",
    "        super(ResNet50, self).__init__()\n",
    "\n",
    "        # Initial Convolutional Layer (same as ResNet-34)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNet-50 Specific Layer Configurations\n",
    "        self.layer1 = self._make_layer(residual_block, 64, 256, 3)  # 3 blocks\n",
    "        self.layer2 = self._make_layer(\n",
    "            residual_block, 256, 512, 4, stride=2\n",
    "        )  # 4 blocks\n",
    "        self.layer3 = self._make_layer(\n",
    "            residual_block, 512, 1024, 6, stride=2\n",
    "        )  # 6 blocks\n",
    "        self.layer4 = self._make_layer(\n",
    "            residual_block, 1024, 2048, 3, stride=2\n",
    "        )  # 3 blocks\n",
    "\n",
    "        # AdaptiveAvgPool2d for any input size\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "        self, residual_block, in_channels, out_channels, num_blocks, stride=1\n",
    "    ):\n",
    "        layers = [residual_block(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(residual_block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data_loader, model, device=\"cpu\"):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "    model.train()  # Set the model back to training mode\n",
    "    return total, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    num_epochs,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    writer,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "):\n",
    "    # Get the model class name dynamically\n",
    "    model_class_name = model.__class__.__name__\n",
    "\n",
    "    # Create the directory based on model class name\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, model_class_name)\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)  # Ensure checkpoint directory exists\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            iter_start = time.time()\n",
    "\n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate the iteration latency\n",
    "            iter_latency = time.time() - iter_start\n",
    "            writer.add_scalar(\n",
    "                \"Latency/iteration\", iter_latency, epoch * len(train_loader) + batch_idx\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"Loss/train\", loss.item(), epoch * len(train_loader) + batch_idx\n",
    "            )\n",
    "\n",
    "            # Clear memory to avoid GPU memory overflow\n",
    "            del images, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # Log epoch latency\n",
    "        epoch_latency = time.time() - epoch_start_time\n",
    "        writer.add_scalar(\"Latency/epoch\", epoch_latency, epoch)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] finished in {epoch_latency:.2f} seconds.\"\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        total, correct = eval(valid_loader, model, device)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Accuracy on validation images after epoch {epoch+1}: {accuracy:.2f}%\")\n",
    "        writer.add_scalar(\"Accuracy/validation\", accuracy, epoch)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Model checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    writer.flush()  # Flush all pending events to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./dataset/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [02:12<00:00, 1280000.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/cifar-100-python.tar.gz to ./dataset\n",
      "Files already downloaded and verified\n",
      "Train size: 45000\n",
      "Validation size: 5000\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"CIFAR100\"\n",
    "num_classes = 100\n",
    "batch_size = 64\n",
    "\n",
    "# Example usage:\n",
    "train_loader, valid_loader = data_loader(\n",
    "    dataset_name=dataset_name, data_dir=\"./dataset\", batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] finished in 319.38 seconds.\n",
      "Accuracy on validation images after epoch 1: 7.28%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_1.pth\n",
      "Epoch [2/50] finished in 2197.37 seconds.\n",
      "Accuracy on validation images after epoch 2: 6.68%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_2.pth\n",
      "Epoch [3/50] finished in 1463.83 seconds.\n",
      "Accuracy on validation images after epoch 3: 9.68%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_3.pth\n",
      "Epoch [4/50] finished in 350.64 seconds.\n",
      "Accuracy on validation images after epoch 4: 10.56%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_4.pth\n",
      "Epoch [5/50] finished in 305.75 seconds.\n",
      "Accuracy on validation images after epoch 5: 6.30%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_5.pth\n",
      "Epoch [6/50] finished in 312.58 seconds.\n",
      "Accuracy on validation images after epoch 6: 11.36%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_6.pth\n",
      "Epoch [7/50] finished in 311.81 seconds.\n",
      "Accuracy on validation images after epoch 7: 9.46%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_7.pth\n",
      "Epoch [8/50] finished in 311.77 seconds.\n",
      "Accuracy on validation images after epoch 8: 15.78%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_8.pth\n",
      "Epoch [9/50] finished in 316.28 seconds.\n",
      "Accuracy on validation images after epoch 9: 8.02%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_9.pth\n",
      "Epoch [10/50] finished in 313.19 seconds.\n",
      "Accuracy on validation images after epoch 10: 16.54%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_10.pth\n",
      "Epoch [11/50] finished in 313.85 seconds.\n",
      "Accuracy on validation images after epoch 11: 16.62%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_11.pth\n",
      "Epoch [12/50] finished in 315.02 seconds.\n",
      "Accuracy on validation images after epoch 12: 14.22%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_12.pth\n",
      "Epoch [13/50] finished in 315.55 seconds.\n",
      "Accuracy on validation images after epoch 13: 17.40%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_13.pth\n",
      "Epoch [14/50] finished in 315.86 seconds.\n",
      "Accuracy on validation images after epoch 14: 16.10%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_14.pth\n",
      "Epoch [15/50] finished in 320.29 seconds.\n",
      "Accuracy on validation images after epoch 15: 13.22%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_15.pth\n",
      "Epoch [16/50] finished in 315.97 seconds.\n",
      "Accuracy on validation images after epoch 16: 12.22%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_16.pth\n",
      "Epoch [17/50] finished in 318.36 seconds.\n",
      "Accuracy on validation images after epoch 17: 14.58%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_17.pth\n",
      "Epoch [18/50] finished in 317.82 seconds.\n",
      "Accuracy on validation images after epoch 18: 15.90%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_18.pth\n",
      "Epoch [19/50] finished in 318.94 seconds.\n",
      "Accuracy on validation images after epoch 19: 13.14%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_19.pth\n",
      "Epoch [20/50] finished in 318.91 seconds.\n",
      "Accuracy on validation images after epoch 20: 14.12%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_20.pth\n",
      "Epoch [21/50] finished in 317.91 seconds.\n",
      "Accuracy on validation images after epoch 21: 13.58%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_21.pth\n",
      "Epoch [22/50] finished in 318.48 seconds.\n",
      "Accuracy on validation images after epoch 22: 20.46%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_22.pth\n",
      "Epoch [23/50] finished in 319.20 seconds.\n",
      "Accuracy on validation images after epoch 23: 12.74%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_23.pth\n",
      "Epoch [24/50] finished in 319.18 seconds.\n",
      "Accuracy on validation images after epoch 24: 14.14%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_24.pth\n",
      "Epoch [25/50] finished in 322.98 seconds.\n",
      "Accuracy on validation images after epoch 25: 19.80%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_25.pth\n",
      "Epoch [26/50] finished in 327.48 seconds.\n",
      "Accuracy on validation images after epoch 26: 8.74%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_26.pth\n",
      "Epoch [27/50] finished in 322.70 seconds.\n",
      "Accuracy on validation images after epoch 27: 17.70%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_27.pth\n",
      "Epoch [28/50] finished in 325.06 seconds.\n",
      "Accuracy on validation images after epoch 28: 18.66%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_28.pth\n",
      "Epoch [29/50] finished in 320.70 seconds.\n",
      "Accuracy on validation images after epoch 29: 19.60%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_29.pth\n",
      "Epoch [30/50] finished in 321.74 seconds.\n",
      "Accuracy on validation images after epoch 30: 15.44%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_30.pth\n",
      "Epoch [31/50] finished in 320.64 seconds.\n",
      "Accuracy on validation images after epoch 31: 19.36%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_31.pth\n",
      "Epoch [32/50] finished in 322.50 seconds.\n",
      "Accuracy on validation images after epoch 32: 13.36%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_32.pth\n",
      "Epoch [33/50] finished in 320.38 seconds.\n",
      "Accuracy on validation images after epoch 33: 17.86%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_33.pth\n",
      "Epoch [34/50] finished in 319.89 seconds.\n",
      "Accuracy on validation images after epoch 34: 16.04%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_34.pth\n",
      "Epoch [35/50] finished in 321.42 seconds.\n",
      "Accuracy on validation images after epoch 35: 11.44%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_35.pth\n",
      "Epoch [36/50] finished in 324.35 seconds.\n",
      "Accuracy on validation images after epoch 36: 15.20%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_36.pth\n",
      "Epoch [37/50] finished in 327.38 seconds.\n",
      "Accuracy on validation images after epoch 37: 9.32%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_37.pth\n",
      "Epoch [38/50] finished in 329.99 seconds.\n",
      "Accuracy on validation images after epoch 38: 5.10%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_38.pth\n",
      "Epoch [39/50] finished in 326.99 seconds.\n",
      "Accuracy on validation images after epoch 39: 19.32%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_39.pth\n",
      "Epoch [40/50] finished in 321.59 seconds.\n",
      "Accuracy on validation images after epoch 40: 17.30%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_40.pth\n",
      "Epoch [41/50] finished in 323.28 seconds.\n",
      "Accuracy on validation images after epoch 41: 10.34%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_41.pth\n",
      "Epoch [42/50] finished in 325.82 seconds.\n",
      "Accuracy on validation images after epoch 42: 8.38%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_42.pth\n",
      "Epoch [43/50] finished in 326.22 seconds.\n",
      "Accuracy on validation images after epoch 43: 9.46%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_43.pth\n",
      "Epoch [44/50] finished in 323.98 seconds.\n",
      "Accuracy on validation images after epoch 44: 19.06%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_44.pth\n",
      "Epoch [45/50] finished in 325.13 seconds.\n",
      "Accuracy on validation images after epoch 45: 20.90%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_45.pth\n",
      "Epoch [46/50] finished in 326.27 seconds.\n",
      "Accuracy on validation images after epoch 46: 18.38%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_46.pth\n",
      "Epoch [47/50] finished in 324.25 seconds.\n",
      "Accuracy on validation images after epoch 47: 17.08%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_47.pth\n",
      "Epoch [48/50] finished in 325.24 seconds.\n",
      "Accuracy on validation images after epoch 48: 16.38%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_48.pth\n",
      "Epoch [49/50] finished in 325.72 seconds.\n",
      "Accuracy on validation images after epoch 49: 18.94%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_49.pth\n",
      "Epoch [50/50] finished in 325.51 seconds.\n",
      "Accuracy on validation images after epoch 50: 12.98%\n",
      "Model checkpoint saved at ./checkpoints/ResNet34/model_epoch_50.pth\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "# model = ResNet34(ResidualBlock_A, num_classes=num_classes).to(device)\n",
    "model = ResNet34(ResidualBlock_A, num_classes=num_classes).to(device)\n",
    "\n",
    "# Define a dummy input to pass through the model\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(\n",
    "    device\n",
    ")  # Example input of shape (batch_size, channels, height, width)\n",
    "\n",
    "# Log the model graph to TensorBoard\n",
    "writer.add_graph(model, dummy_input)\n",
    "writer.flush()\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     model.parameters(), lr=learning_rate, weight_decay=0.001, momentum=0.9\n",
    "# )\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "\n",
    "# Learning rate scheduler (reduce LR by 0.1 every 10 epochs)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "train(\n",
    "    num_epochs=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    writer=writer,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_loader = data_loader(\n",
    "    dataset_name=dataset_name, data_dir=\"./dataset\", batch_size=batch_size, test=True\n",
    ")\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "total, correct = eval(test_loader, model, device)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy of the network on the {total} test images: {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
